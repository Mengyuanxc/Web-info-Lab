## <center> 实验1实验报告
<font size = 1> <center> PB21000057 顾荣建 &emsp; PB21000069 林润锋 &emsp; PB21000074 田钦 </center></font>

### 第一阶段.豆瓣数据的爬取与检索
#### 1.0 实验要求:
对于指定ID的电影、书籍，爬取其主页并解析其基本信息，然后结合给定的标签信息， 实现电影和书籍的检索并评估其效果，对于给定的查询，能够以精确查询或模糊语义匹配的方法返回最相关的书籍或者电影集。
#### 1.1 爬虫
##### 1.1.1 爬虫要求：
针对给定的电影、书籍ID，爬取其基本信息、简介、演员/作者等，选取网页爬取或API爬取方式，解析网页内容并提交所获数据。
##### 1.1.2 所爬取信息:
**电影**:电影标题、电影评分、导演、演员、电影类型、上映日期、电影简介、"喜欢这部电影的人也喜欢“。
**书籍**:To be done.
##### 1.1.3 爬虫实现:
###### 1)采用python语言的requests库爬取网页，用bs4.BeautifulSoup库对爬取信息进行整理
###### 2)爬虫方式:网页爬取
###### 3)平台反爬措施:如果同一个IP频繁对豆瓣发出请求，那么豆瓣会封禁该IP
###### 4)应对措施:一开始采用了隔几十秒(sleep(randit(40,60)))爬取一部电影的方式来避免被封禁IP，后面发现一个IP要频繁访问几百次才会被封禁，于是就取消了sleep，改为每爬取几百部电影就换一个IP继续爬的方式
##### 1.1.4 爬虫代码(以爬取电影为例):
###### 1)获取网页:
```py
    url = f'https://movie.douban.com/subject/{movie_id}/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)
         AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
```
###### 2)信息提取:
```py
    # 获取电影标题
    chinese_title = soup.title.get_text().strip()[:-5]
    title_element = soup.select_one('h1 span[property="v:itemreviewed"]')
    title = title_element.text.strip() if title_element else ""

    # 获取电影评分
    rating_element = soup.select_one('strong.ll.rating_num')
    rating = rating_element.text.strip() if rating_element else "无"
    if(rating!="无"):
        rating = float(rating)

    # 获取导演
    director_elements = soup.select('a[rel="v:directedBy"]')
    directors = [director.text for director in director_elements]
```
###### 3)csv文件读ID并调用爬虫函数:
```py
    # 调用函数并输入豆瓣电影的ID和文件路径
    name = "Movie_details.csv"
    cnt = 0
    with open(name, 'r', encoding='utf-8') as f:   #跳过已经爬下的部分
        line = f.readline()
        while(line):
            cnt+= 1
            line = f.readline()

    with open('Movie_id.csv', 'r') as f:        #根据ID爬取对应电影
        line = f.readline()
        while(cnt>0):
            cnt-=1
            line = f.readline()
        while line:
            print(line)
            save_movie_details(line[:-1], name)
            line = f.readline()
            #sleep(random.randint(20, 40))
```
###### 4)信息存入文件:
```py
    with open(file_path, 'a', encoding='utf-8',newline="") as file:
        csv_writer = csv.writer(file)
        z = [
            movie_id,
            chinese_title,
            title,
            rating,
            directors,
            actors,
            genres,
            release_date,
            summary,
            recommendation_list
        ]
        csv_writer.writerow(z)
        print("写入数据成功")
        file.close()
```
#### 1.2 信息检索
##### 1.2.1 检索要求:
实现电影、书籍的bool检索，对内容简介进行分词、去停用词处理，表征为一系列关键词集合，建立倒排表来实现检索。
##### 1.2.2 检索关键词提取:
**电影**: 电影关键词提取整合自导演、演员、电影类型以及剧情简介的分词
**书籍**: To be done
##### 1.2.3 所用分词工具及其差异:
###### 我们组所采用的分词工具为jieba和snownlp，结果发现jieba要远远优于snownlp.
以电影为例，在分词用时上，jieba对1200部电影的剧情简介完成分词用时仅为3s，而snownlp则用时为100s，从准确度看，jieba分词也要远优于snownlp，例如在肖申克的救赎剧情简介开头中，两者分词分别为：
**jieba**:'一场', '谋杀案', '使', '银行家', '安迪'
**snownlp**:'一', '场', '谋杀', '案使', '银行家', '安迪'
通过观察整个分词结果，也可以明显发现jieba分词的准确率比snownlp准确率高很多
##### 1.2.4
