## <center> 实验1实验报告
<font size = 1> <center> PB21000057 顾荣建 &emsp; PB21000069 林润锋 &emsp; PB21000074 田钦 </center></font>

### 第一阶段.豆瓣数据的爬取与检索
#### 1.0 实验要求:
对于指定ID的电影、书籍，爬取其主页并解析其基本信息，然后结合给定的标签信息， 实现电影和书籍的检索并评估其效果，对于给定的查询，能够以精确查询或模糊语义匹配的方法返回最相关的书籍或者电影集。
#### 1.1 爬虫
##### 1.1.1 爬虫要求：
针对给定的电影、书籍ID，爬取其基本信息、简介、演员/作者等，选取网页爬取或API爬取方式，解析网页内容并提交所获数据。
##### 1.1.2 所爬取信息:
**电影**:电影标题、电影评分、导演、演员、电影类型、上映日期、电影简介、"喜欢这部电影的人也喜欢“。
**书籍**:书籍标题、副标题、评分、作者、出版社、原名（对于外文书籍）、ISBN号、译者、出版时间、页码、价格、简介

##### 1.1.3 爬虫实现:
###### 1)采用python语言的requests库爬取网页，用bs4.BeautifulSoup库对爬取信息进行整理
###### 2)爬虫方式:网页爬取
###### 3)平台反爬措施:如果同一个IP频繁对豆瓣发出请求，那么豆瓣会封禁该IP
###### 4)应对措施:一开始采用了隔几十秒(sleep(randit(40,60)))爬取一部电影的方式来避免被封禁IP，后面发现一个IP要频繁访问几百次才会被封禁，于是就取消了sleep，改为每爬取几百部电影就换一个IP继续爬的方式
##### 1.1.4 爬虫代码:

`BeautifulSoup`库可以向网页发送访问请求从而得到网页等返回的html信息，并且其中提供的库函数可以对html源码进行解析，如`soup.select()`函数，可以在html源码中寻找特定的元素，可以根据类型、内容等进行筛选，具体实现如下(以爬取电影为例)：

###### 1)获取网页:
```py
    url = f'https://movie.douban.com/subject/{movie_id}/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)
         AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
```
###### 2)信息提取:
```py
    # 获取电影标题
    chinese_title = soup.title.get_text().strip()[:-5]
    title_element = soup.select_one('h1 span[property="v:itemreviewed"]')
    title = title_element.text.strip() if title_element else ""

    # 获取电影评分
    rating_element = soup.select_one('strong.ll.rating_num')
    rating = rating_element.text.strip() if rating_element else "无"
    if(rating!="无"):
        rating = float(rating)

    # 获取导演
    director_elements = soup.select('a[rel="v:directedBy"]')
    directors = [director.text for director in director_elements]
```
###### 3)csv文件读ID并调用爬虫函数:
```py
    # 调用函数并输入豆瓣电影的ID和文件路径
    name = "Movie_details.csv"
    cnt = 0
    with open(name, 'r', encoding='utf-8') as f:   #跳过已经爬下的部分
        line = f.readline()
        while(line):
            cnt+= 1
            line = f.readline()

    with open('Movie_id.csv', 'r') as f:        #根据ID爬取对应电影
        line = f.readline()
        while(cnt>0):
            cnt-=1
            line = f.readline()
        while line:
            print(line)
            save_movie_details(line[:-1], name)
            line = f.readline()
            #sleep(random.randint(20, 40))
```
###### 4)信息存入文件:
```py
    with open(file_path, 'a', encoding='utf-8',newline="") as file:
        csv_writer = csv.writer(file)
        z = [
            movie_id,
            chinese_title,
            title,
            rating,
            directors,
            actors,
            genres,
            release_date,
            summary,
            recommendation_list
        ]
        csv_writer.writerow(z)
        print("写入数据成功")
        file.close()
```
#### 1.2 信息检索
##### 1.2.1 检索要求:
实现电影、书籍的bool检索，对内容简介进行分词、去停用词处理，表征为一系列关键词集合，建立倒排表来实现检索。
##### 1.2.2 检索关键词提取:
**电影**: 电影关键词提取整合自导演、演员、电影类型以及剧情简介的分词
**书籍**: 书籍关键词提取整合自作者、标题以及书籍简介等的分词

##### 1.2.3 所用分词工具及其差异:
###### 我们组所采用的分词工具为jieba和snownlp，结果发现jieba要远远优于snownlp.
以电影为例，在分词用时上，jieba对1200部电影的剧情简介完成分词用时仅为3s，而snownlp则用时为100s，从准确度看，jieba分词也要远优于snownlp，例如在肖申克的救赎剧情简介开头中，两者分词分别为：
**jieba**:'一场', '谋杀案', '使', '银行家', '安迪'
**snownlp**:'一', '场', '谋杀', '案使', '银行家', '安迪'
通过观察整个分词结果，也可以明显发现jieba分词的准确率比snownlp准确率高很多
##### 1.2.4 索引压缩优化

这里采用间隔存储+可变字节长度的方式对于倒排表索引进行压缩优化以减小存储需求

基本实现步骤如下（对于倒排表中某一项的列表a[]而言）：

1. 首先将每一项，将它改写为它和上一项的差

2. 改写为可变字长编码，由于我们只有1200个数据项，最多只需要两个字节即可存储，可以进行简化。

   若id<128，则用一个字节b=128+id存储；

   若id>=128，则用两个字节b1=128+(id>>7)，b0=id%128存储

##### 1.2.5 查询

对于任意合法的查询表达式（包含关键词以及三种逻辑运算符：`AND`/`OR`/`NOT`），我们可以先将它转化为对应的主析取范式，对于析取范式中的每一项，得到对应目标的得分，取一个目标在析取范式中的各项取得的最高分作为它自身的得分，最后进行比较，得到得分最高（也就是和查询条件匹配度）最高的前若干项

对于析取范式中的项，它一定由若干项合取得到，那么我们用匹配项/总项数得到这一合取式的得分，特别的，若某项存在`NOT`，则出现关键词不得分，不出现关键词则得分

