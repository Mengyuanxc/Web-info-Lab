{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:46:31.888963500Z",
     "start_time": "2023-11-05T11:46:31.880274100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成文本表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:46:34.969975600Z",
     "start_time": "2023-11-05T11:46:32.852045Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:11.446700400Z",
     "start_time": "2023-11-05T11:46:35.743219500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:35, 33.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 读loaded_data取保存的 CSV 文件\n",
    "loaded_data = pd.read_csv('data\\selected_book_top_1200_data_tag.csv')\n",
    "\n",
    "tag_embedding_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, rows in tqdm(loaded_data.iterrows()):\n",
    "        # 将标签列表转换为字符串\n",
    "        tags_str = \" \".join(rows.Tags)\n",
    "        # 使用BERT中文模型对标签进行编码\n",
    "        inputs = tokenizer(tags_str, truncation=True, return_tensors='pt')\n",
    "        outputs = model(inputs.input_ids.cuda(), inputs.token_type_ids.cuda(), inputs.attention_mask.cuda())\n",
    "        # 使用最后一层的平均隐藏状态作为标签的向量表示\n",
    "        tag_embedding = outputs.last_hidden_state.mean(dim=1).cpu()\n",
    "        tag_embedding_dict[rows.Book] = tag_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:20.355722100Z",
     "start_time": "2023-11-05T11:47:20.285308200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 将映射表存储为二进制文件\n",
    "with open('data/tag_embedding_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tag_embedding_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:22.731254100Z",
     "start_time": "2023-11-05T11:47:22.638641Z"
    }
   },
   "outputs": [],
   "source": [
    "# 从二进制文件中读取映射表\n",
    "with open('data/tag_embedding_dict.pkl', 'rb') as f:\n",
    "    tag_embedding_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:24.118347800Z",
     "start_time": "2023-11-05T11:47:23.431434600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           User     Book  Rate                       Time         Tag\n",
      "0       1398478  1467022     0  2011-03-29T12:48:35+08:00         NaN\n",
      "1       1398478  1777823     0  2011-02-02T21:58:55+08:00         NaN\n",
      "2       1398478  1902628     0  2011-01-31T15:57:58+08:00         NaN\n",
      "3       1398478  1878708     0  2011-01-26T11:27:59+08:00         NaN\n",
      "4       1398478  4238362     0  2011-01-21T13:04:15+08:00         NaN\n",
      "...         ...      ...   ...                        ...         ...\n",
      "637249  4507957  1125186     4  2009-07-04T08:02:13+08:00  张爱玲,半生缘,爱情\n",
      "637250  4507957  1002299     5  2009-07-04T08:01:28+08:00  金庸,武侠,笑傲江湖\n",
      "637251  4507957  1001136     4  2009-07-04T07:55:17+08:00     彼得・潘,童话\n",
      "637252  4507957  1021615     5  2009-07-04T07:53:54+08:00   小王子,童话,经典\n",
      "637253  4507957  1962929     5  2009-06-29T22:13:37+08:00          爱情\n",
      "\n",
      "[637254 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 读loaded_data取保存的 CSV 文件\n",
    "loaded_data = pd.read_csv('data\\\\book_score.csv')\n",
    "\n",
    "# 显示加载的数据\n",
    "print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:27.154760600Z",
     "start_time": "2023-11-05T11:47:27.149430500Z"
    }
   },
   "outputs": [],
   "source": [
    "class BookRatingDataset(Dataset):\n",
    "    def __init__(self, data, user_to_idx, book_to_idx, tag_embedding_dict):\n",
    "        self.data = data\n",
    "        self.user_to_idx = user_to_idx\n",
    "        self.book_to_idx = book_to_idx\n",
    "        self.tag_embedding_dict = tag_embedding_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        user = self.user_to_idx[row['User']]\n",
    "        book = self.book_to_idx[row['Book']]\n",
    "        rating = row['Rate'].astype('float32')\n",
    "        text_embedding = self.tag_embedding_dict.get(row['Book'])\n",
    "        return user, book, rating, text_embedding\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_books, embedding_dim, hidden_state):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.book_embeddings = nn.Embedding(num_books, embedding_dim)\n",
    "        self.linear_embedding = nn.Linear(hidden_state, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, 6)\n",
    "\n",
    "    def forward(self, user, book, tag_embedding):\n",
    "        user_embedding = self.user_embeddings(user)\n",
    "        book_embedding = self.book_embeddings(book)\n",
    "        tag_embedding_proj = self.linear_embedding(tag_embedding)\n",
    "        book_intergrate = book_embedding + tag_embedding_proj\n",
    "        return (user_embedding * book_intergrate).sum(dim = 1)\n",
    "        \n",
    "def create_id_mapping(id_list):\n",
    "    # 从ID列表中删除重复项并创建一个排序的列表\n",
    "    unique_ids = sorted(set(id_list))\n",
    "    \n",
    "    # 创建将原始ID映射到连续索引的字典\n",
    "    id_to_idx = {id: idx for idx, id in enumerate(unique_ids)}\n",
    "    \n",
    "    # 创建将连续索引映射回原始ID的字典\n",
    "    idx_to_id = {idx: id for id, idx in id_to_idx.items()}\n",
    "    \n",
    "    return id_to_idx, idx_to_id\n",
    "\n",
    "# 按用户分组计算NDCG\n",
    "def compute_ndcg(group):\n",
    "    true_ratings = group['true'].tolist()\n",
    "    pred_ratings = group['pred'].tolist()\n",
    "    return ndcg_score([true_ratings], [pred_ratings], k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:47:28.564874600Z",
     "start_time": "2023-11-05T11:47:27.808743Z"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = loaded_data['User'].unique()\n",
    "book_ids = loaded_data['Book'].unique()\n",
    "\n",
    "user_to_idx, idx_to_user = create_id_mapping(user_ids)\n",
    "book_to_idx, idx_to_book = create_id_mapping(book_ids)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_data, test_data = train_test_split(loaded_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# 创建训练集和测试集的数据集对象\n",
    "train_dataset = BookRatingDataset(train_data, user_to_idx, book_to_idx, tag_embedding_dict)\n",
    "test_dataset = BookRatingDataset(test_data, user_to_idx, book_to_idx, tag_embedding_dict)\n",
    "\n",
    "# 创建训练集和测试集的数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4096, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4096, shuffle=False, drop_last = True)\n",
    "\n",
    "num_users = loaded_data['User'].nunique()  \n",
    "num_books = loaded_data['Book'].nunique() \n",
    "embedding_dim, hidden_state = 32, 768\n",
    "\n",
    "model = MatrixFactorization(num_users, num_books, embedding_dim, hidden_state).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T11:59:31.775621Z",
     "start_time": "2023-11-05T11:47:30.082798200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 58.44987188066755, Test loss:, 31.881399303287655, Average NDCG: 0.6691490207651726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:21,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 26.447907509741846, Test loss:, 24.352959447092825, Average NDCG: 0.6694565123436891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:18,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 19.125587091817483, Test loss:, 19.20553915841239, Average NDCG: 0.6702010034166034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 13.293060810534985, Test loss:, 14.793430613232898, Average NDCG: 0.672327004063418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 8.136883512719885, Test loss:, 11.658517726055987, Average NDCG: 0.6738796833446161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 5.576303618294852, Test loss:, 10.145865465139414, Average NDCG: 0.6753202383013844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 4.350680190247375, Test loss:, 9.319239120978814, Average NDCG: 0.6767742468734121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train loss: 3.6851120954984196, Test loss:, 8.700458811475086, Average NDCG: 0.6786532710412566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 3.28783300325468, Test loss:, 8.007065618192994, Average NDCG: 0.6803549408853783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 3.0118476601390096, Test loss:, 7.9892864970417765, Average NDCG: 0.6809873403088447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train loss: 2.8402751575816763, Test loss:, 7.605630261557443, Average NDCG: 0.6837529611399864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 2.6760646553782674, Test loss:, 7.4857153954444, Average NDCG: 0.6847144970736759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 2.6270086610472045, Test loss:, 7.284227587959983, Average NDCG: 0.6865595457467792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train loss: 2.5629109531253964, Test loss:, 7.358892242629807, Average NDCG: 0.6872959258917418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train loss: 2.5732413514868004, Test loss:, 7.284316583113237, Average NDCG: 0.6880969162213166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train loss: 2.4762384829583106, Test loss:, 7.185438806360418, Average NDCG: 0.6897601388852876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train loss: 2.467888259268426, Test loss:, 7.017297051169655, Average NDCG: 0.6897505439998481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train loss: 2.4498681836314016, Test loss:, 7.52491264838677, Average NDCG: 0.6908313927710616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train loss: 2.4967031757552904, Test loss:, 7.407335250408618, Average NDCG: 0.6909603122180712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:17,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train loss: 2.4321695055280412, Test loss:, 7.198828294679716, Average NDCG: 0.6913566949779979\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "lambda_u, lambda_b = 0.001, 0.001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss_train, total_loss_test = 0.0, 0.0\n",
    "\n",
    "    for idx, (user_ids, book_ids, ratings, tag_embedding) in tqdm(enumerate(train_dataloader)):\n",
    "        # 使用user_ids, book_ids, ratings进行训练\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(user_ids.to(device), book_ids.to(device), tag_embedding.squeeze(1).to(device))\n",
    "        loss = criterion(predictions, ratings.to(device)) + lambda_u * model.user_embeddings.weight.norm(2) + lambda_b * model.book_embeddings.weight.norm(2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        # if idx % 100 == 0:\n",
    "        #     print(f'Step {idx}, Loss: {loss.item()}')\n",
    "\n",
    "    output_loss_train = total_loss_train / (idx + 1) \n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (user_ids, item_ids, true_ratings, tag_embedding) in enumerate(test_dataloader):\n",
    "            pred_ratings = model(user_ids.to(device), item_ids.to(device), tag_embedding.squeeze(1).to(device))\n",
    "\n",
    "            loss = criterion(pred_ratings, ratings.to(device))\n",
    "            total_loss_test += loss.item()\n",
    "\n",
    "            # 将结果转换为 numpy arrays\n",
    "            user_ids_np = user_ids.long().cpu().numpy().reshape(-1, 1)\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy().reshape(-1, 1)\n",
    "            true_ratings_np = true_ratings.numpy().reshape(-1, 1)\n",
    "\n",
    "            # 将这三个 arrays 合并成一个 2D array\n",
    "            batch_results = np.column_stack((user_ids_np, pred_ratings_np, true_ratings_np))\n",
    "\n",
    "            # 将这个 2D array 添加到 results\n",
    "            results.append(batch_results)\n",
    "\n",
    "        # 将结果的 list 转换为一个大的 numpy array\n",
    "        results = np.vstack(results)\n",
    "\n",
    "        # 将结果转换为DataFrame\n",
    "        results_df = pd.DataFrame(results, columns=['user', 'pred', 'true'])\n",
    "        results_df['user'] = results_df['user'].astype(int)\n",
    "\n",
    "        ndcg_scores = results_df.groupby('user').apply(compute_ndcg)\n",
    "\n",
    "        # 计算平均NDCG\n",
    "        avg_ndcg = ndcg_scores.mean()\n",
    "        print(f'Epoch {epoch}, Train loss: {output_loss_train}, Test loss:, {total_loss_test / (idx + 1)}, Average NDCG: {avg_ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
